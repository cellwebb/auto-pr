# Auto-PR Configuration Example

# REQUIRED - Fully qualified model in format 'provider:model'
# AUTO_PR_MODEL=claude-code:claude-sonnet-4-5
# AUTO_PR_MODEL=custom-anthropic:claude-haiku-4-5
# AUTO_PR_MODEL=custom-openai:gpt-4
# AUTO_PR_MODEL=deepseek:deepseek-chat
# AUTO_PR_MODEL=groq:meta-llama/llama-4-scout-17b-16e-instruct
# AUTO_PR_MODEL=kimi-for-coding:kimi-coding
# AUTO_PR_MODEL=minimax:abab6.5s-chat
# AUTO_PR_MODEL=moonshot:moonshot-v1-8k
# AUTO_PR_MODEL=mistral:mistral-large-latest
# AUTO_PR_MODEL=replicate:openai/gpt-oss-120b
# AUTO_PR_MODEL=together:meta-llama/Llama-3.2-3B-Instruct-Turbo

# AI Provider API Keys
# Uncomment and add your key for the provider(s) you want to use
# ANTHROPIC_API_KEY=your_key_here
# CEREBRAS_API_KEY=your_key_here
# CLAUDE_CODE_ACCESS_TOKEN=your_token_here  # OAuth token for Claude Code subscription users
# CUSTOM_ANTHROPIC_API_KEY=your_key_here  # For custom Anthropic-compatible endpoints
# CUSTOM_OPENAI_API_KEY=your_key_here  # For custom OpenAI-compatible endpoints (e.g., Azure OpenAI)
# DEEPSEEK_API_KEY=your_key_here
# GEMINI_API_KEY=your_key_here
# GROQ_API_KEY=your_key_here
# KIMI_API_KEY=your_key_here
# LMSTUDIO_API_KEY=your_key_here  # Optional - only if your LM Studio instance requires authentication
# MINIMAX_API_KEY=your_key_here
# MOONSHOT_API_KEY=your_key_here
# MISTRAL_API_KEY=your_key_here
# OLLAMA_API_KEY=your_key_here  # Optional - only if your Ollama instance requires authentication
# OPENAI_API_KEY=your_key_here
# OPENROUTER_API_KEY=your_key_here
# REPLICATE_API_TOKEN=your_token_here
# SYNTHETIC_API_KEY=your_key_here
# TOGETHER_API_KEY=your_key_here
# ZAI_API_KEY=your_key_here

# AI Provider URLs
# Uncomment and customize if you're using custom endpoints
# CUSTOM_ANTHROPIC_BASE_URL=https://your-proxy.example.com  # Required for custom-anthropic provider
# CUSTOM_ANTHROPIC_VERSION=2023-06-01  # Optional - API version header for custom-anthropic
# CUSTOM_OPENAI_BASE_URL=https://your-endpoint.example.com  # Required for custom-openai provider
# LMSTUDIO_API_URL=http://localhost:1234  # LM Studio server URL
# OLLAMA_API_URL=http://localhost:11434  # Ollama server URL


# OPTIONAL - Auto-PR Settings
# AUTO_PR_WARNING_LIMIT_TOKENS=128000
# AUTO_PR_MAX_OUTPUT_TOKENS=512
# AUTO_PR_TEMPERATURE=0.7
# AUTO_PR_ALWAYS_INCLUDE_SCOPE=true
# AUTO_PR_VERBOSE=true  # Generate detailed PR descriptions with testing instructions and impact analysis
# AUTO_PR_NO_TIKTOKEN=true  # Skip tiktoken downloads and use offline, approximate token counting
# AUTO_PR_ZAI_USE_CODING_PLAN=false  # Set to true to use coding API endpoint instead of regular API

# OPTIONAL - Custom System Prompt
# Path to a custom system prompt file that defines how PR descriptions should be generated
# Write plain text instructions - no special format or tags required
# See custom_system_prompt.example.txt for an example
# The user prompt (git diff data) is automatically added and not customizable
# AUTO_PR_SYSTEM_PROMPT_PATH=/path/to/custom_system_prompt.txt

# OPTIONAL - PR Description Language
# Language for generated PR descriptions (e.g., Spanish, French, Japanese, German, Portuguese)
# Use 'auto-pr language' for an interactive selector with 25+ languages
# Default: English
# AUTO_PR_LANGUAGE=Spanish

# OPTIONAL - Translate Conventional Commit Prefixes
# When true, translates conventional commit prefixes (feat, fix, docs, etc.) into the target language
# When false (default), keeps prefixes in English for tool compatibility (changelog generators, etc.)
# Only applies when AUTO_PR_LANGUAGE is set
# Default: false
# AUTO_PR_TRANSLATE_PREFIXES=true

# OPTIONAL - RTL Language Confirmation
# When true, suppresses the RTL warning when selecting right-to-left languages
# Automatically set after first RTL language selection to avoid repeated warnings
# Default: false
# AUTO_PR_RTL_CONFIRMED=true
